% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ParseSettings.R
\name{.parse_evaluation_settings}
\alias{.parse_evaluation_settings}
\title{Internal function for parsing settings related to model evaluation}
\usage{
.parse_evaluation_settings(
  config = NULL,
  data,
  parallel,
  outcome_type,
  hpo_metric,
  development_batch_id,
  vimp_aggregation_method,
  vimp_aggregation_rank_threshold,
  prep_cluster_method,
  prep_cluster_linkage_method,
  prep_cluster_cut_method,
  prep_cluster_similarity_threshold,
  prep_cluster_similarity_metric,
  evaluate_top_level_only = waiver(),
  ensemble_method = waiver(),
  evaluation_metric = waiver(),
  confidence_level = waiver(),
  bootstrap_ci_method = waiver(),
  compute_model_data = waiver(),
  compute_model_ci = waiver(),
  compute_ensemble_ci = waiver(),
  aggregate_ci = waiver(),
  feature_cluster_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_linkage_method = waiver(),
  feature_similarity_metric = waiver(),
  feature_similarity_threshold = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  sample_similarity_metric = waiver(),
  eval_aggregation_method = waiver(),
  eval_aggregation_rank_threshold = waiver(),
  eval_icc_type = waiver(),
  stratification_method = waiver(),
  stratification_threshold = waiver(),
  stratification_ensemble_method = waiver(),
  time_max = waiver(),
  evaluation_times = waiver(),
  parallel_evaluation = waiver(),
  ...
)
}
\arguments{
\item{config}{A list of settings, e.g. from an xml file.}

\item{data}{Data set as loaded using the \code{.load_data} function.}

\item{parallel}{Logical value that whether familiar uses parallelisation. If
\code{FALSE} it will override \code{parallel_evaluation}.}

\item{outcome_type}{Type of outcome found in the data set.}

\item{hpo_metric}{Metric defined for hyperparameter optimisation.}

\item{development_batch_id}{Identifiers of batches used for model development.
These identifiers are used to determine the cohorts used to determine a
setting for \code{time_max}, if the \code{outcome_type} is \code{survival}, and both
\code{time_max} and \code{evaluation_times} are not provided.}

\item{vimp_aggregation_method}{Method for variable importance aggregation that
was used for feature selection.}

\item{vimp_aggregation_rank_threshold}{Rank threshold for variable importance
aggregation used during feature selection.}

\item{prep_cluster_method}{Cluster method used during pre-processing.}

\item{prep_cluster_linkage_method}{Cluster linkage method used during
pre-processing.}

\item{prep_cluster_cut_method}{Cluster cut method used during pre-processing.}

\item{prep_cluster_similarity_threshold}{Cluster similarity threshold used
during pre-processing.}

\item{prep_cluster_similarity_metric}{Cluster similarity metric used during
pre-processing.}

\item{evaluate_top_level_only}{(\emph{optional}) Flag that signals that only
evaluation at the most global experiment level is required. Consider a
cross-validation experiment with additional external validation. The global
experiment level consists of data that are used for development, internal
validation and external validation. The next lower experiment level are the
individual cross-validation iterations.

When the flag is \code{true}, evaluations take place on the global level only,
and no results are generated for the next lower experiment levels. In our
example, this means that results from individual cross-validation iterations
are not computed and shown. When the flag is \code{false}, results are computed
from both the global layer and the next lower level.

Setting the flag to \code{true} saves computation time.}

\item{ensemble_method}{(\emph{optional}) Method for ensembling predictions from
models for the same sample. Available methods are:
\itemize{
\item \code{median} (default): Use the median of the predicted values as the ensemble
value for a sample.
\item \code{mean}: Use the mean of the predicted values as the ensemble value for a
sample.
}}

\item{evaluation_metric}{(\emph{optional}) One or more metrics for assessing model
performance. See the vignette on performance metrics for the available
metrics.

Confidence intervals (or rather credibility intervals) are computed for each
metric during evaluation. This is done using bootstraps, the number of which
depends on the value of \code{confidence_level} (Davison and Hinkley, 1997).

If unset, the metric in the \code{optimisation_metric} variable is used.}

\item{confidence_level}{(\emph{optional}) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, \code{familiar} uses the
rule of thumb \eqn{n = 20 / ci.level} to determine the number of required
bootstraps.

The default value is \code{0.95}.}

\item{bootstrap_ci_method}{(\emph{optional}) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
\itemize{
\item \code{percentile}: Confidence intervals obtained using the percentile method.
\item \code{bc} (default): Bias-corrected confidence intervals.
}

Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.}

\item{compute_model_data}{(\emph{optional}) This parameter can be set to enable
computation of data based on individual models. The parameter can take on or
more of the following values: \code{all}, \code{model_performance}, \code{auc_data},
\code{confusion_matrix}, \code{decision_curve_analyis}, \code{permutation_vimp},
\code{performance_data}, as well as \code{true}, \code{false} and \code{none}.

By default, data is computed for the ensemble as a whole, but not for
underlying models.}

\item{compute_model_ci}{(\emph{optional}) This parameter can be set to enable
computation of bootstrap confidence intervals for individual models in
several parts of the evaluation. The parameter can take one or more of the
following values: \code{all}, \code{model_performance}, \code{auc_data},
\code{decision_curve_analyis}, \code{permutation_vimp} as well as \code{true}, \code{false} and
\code{none}.

By default, bootstrap confidence intervals are not computed for individual
models. Note that this parameter has no effect unless data is computed for
individual models as well, which is managed by the \code{compute_model_data}
parameter.}

\item{compute_ensemble_ci}{(\emph{optional}) This parameter can be set to enable
computation of bootstrap confidence intervals for ensemble models in several
parts of the evaluation. The parameter can take one or more of the following
values: \code{all}, \code{model_performance}, \code{auc_data}, \code{decision_curve_analyis},
\code{permutation_vimp} as well as \code{true}, \code{false} and \code{none}.

By default, bootstrap confidence intervals are computed for ensemble models.}

\item{aggregate_ci}{(\emph{optional}) Bootstraps are used to determine confidence
intervals. This information can be stored for export. However, in many cases
this is not necessary, and keeping the bootstrap data can lead to large
\code{familiarData} and \code{familiarCollection} objects. This provides the option to
aggregate the bootstrap data by computing the confidence interval directly.

This parameter can take one or more of the following values: \code{all},
\code{model_performance}, \code{auc_data}, \code{decision_curve_analyis},
\code{permutation_vimp} as well as \code{true}, \code{false} and \code{none}. By default,
bootstrap data is aggregated by computing confidence intervals for
receiver-operating characteristic curves, decision curves and permutation
variable importance.}

\item{feature_cluster_method}{(\emph{optional}) Method used to perform clustering
of features. The same methods as for the \code{cluster_method} configuration
parameter are available: \code{none}, \code{hclust}, \code{agnes}, \code{diana} and \code{pam}.

The value for the \code{cluster_method} configuration parameter is used by
default. When generating clusters for the purpose of determining mutual
correlation and ordering feature expressions, \code{none} is ignored and \code{hclust}
is used instead.}

\item{feature_cluster_cut_method}{(\emph{optional}) Method used to divide features
into separate clusters. The available methods are the same as for the
\code{cluster_cut_method} configuration parameter: \code{silhouette}, \code{fixed_cut} and
\code{dynamic_cut}.

\code{silhouette} is available for all cluster methods, but \code{fixed_cut} only
applies to methods that create hierarchical trees (\code{hclust}, \code{agnes} and
\code{diana}). \code{dynamic_cut} requires the \code{dynamicTreeCut} package and can only
be used with \code{agnes} and \code{hclust}.

The value for the \code{cluster_cut_method} configuration parameter is used by
default.}

\item{feature_linkage_method}{(\emph{optional}) Method used for agglomerative
clustering with \code{hclust} and \code{agnes}. Linkage determines how features are
sequentially combined into clusters based on distance. The methods are
shared with the \code{cluster_linkage_method} configuration parameter: \code{average},
\code{single}, \code{complete}, \code{weighted}, and \code{ward}.

The value for the \code{cluster_linkage_method} configuration parameters is used
by default.}

\item{feature_similarity_metric}{(\emph{optional}) Metric to determine pairwise
similarity between features. Similarity is computed in the same manner as
for clustering, and \code{feature_similarity_metric} therefore has the same
options as \code{cluster_similarity_metric}: \code{mcfadden_r2}, \code{cox_snell_r2},
\code{nagelkerke_r2}, \code{spearman}, \code{kendall} and \code{pearson}.

The value used for the \code{cluster_similarity_metric} configuration parameter
is used by default.}

\item{feature_similarity_threshold}{(\emph{optional}) The threshold level for
pair-wise similarity that is required to form feature clusters with the
\code{fixed_cut} method. This threshold functions in the same manner as the one
defined using the \code{cluster_similarity_threshold} parameter.

By default, the value for the \code{cluster_similarity_threshold} configuration
parameter is used.

Unlike for \code{cluster_similarity_threshold}, more than one value can be
supplied here.}

\item{sample_cluster_method}{(\emph{optional}) The method used to perform
clustering based on distance between samples. These are the same methods as
for the \code{cluster_method} configuration parameter: \code{hclust}, \code{agnes}, \code{diana}
and \code{pam}.

The value for the \code{cluster_method} configuration parameter is used by
default. When generating clusters for the purpose of ordering samples in
feature expressions, \code{none} is ignored and \code{hclust} is used instead.}

\item{sample_linkage_method}{(\emph{optional}) The method used for agglomerative
clustering in \code{hclust} and \code{agnes}. These are the same methods as for the
\code{cluster_linkage_method} configuration parameter: \code{average}, \code{single},
\code{complete}, \code{weighted}, and \code{ward}.

The value for the \code{cluster_linkage_method} configuration parameters is used
by default.}

\item{sample_similarity_metric}{(\emph{optional}) Metric to determine pairwise
similarity between samples. Similarity is computed in the same manner as for
clustering, but \code{sample_similarity_metric} has different options that are
better suited to computing distance between samples instead of between
features. The following metrics are available.
\itemize{
\item \code{gower} (default): compute Gower's distance between samples. By default,
Gower's distance is computed based on winsorised data to reduce the effect
of outliers (see below).
\item \code{euclidean}: compute the Euclidean distance between samples.
}

The underlying feature data for numerical features is scaled to the
\eqn{[0,1]} range using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5\% (on both sides) of feature values trimmed or
winsorised. To do so append \verb{_trim} (trimming) or \verb{_winsor} (winsorising) to
the metric name. This reduces the effect of outliers somewhat.

Regardless of metric, all categorical features are handled as for the
Gower's distance: distance is 0 if the values in a pair of samples match,
and 1 if they do not.}

\item{eval_aggregation_method}{(\emph{optional}) Method for aggregating variable
importances for the purpose of evaluation. Variable importances are
determined during feature selection steps and after training the model. Both
types are evaluated, but feature selection variable importance is only
evaluated at run-time.

See the documentation for the \code{vimp_aggregation_method} argument for
information concerning the different methods available.}

\item{eval_aggregation_rank_threshold}{(\emph{optional}) The threshold used to
define the subset of highly important features during evaluation.

See the documentation for the \code{vimp_aggregation_rank_threshold} argument for
more information.}

\item{eval_icc_type}{(\emph{optional}) String indicating the type of intraclass
correlation coefficient (\code{1}, \code{2} or \code{3}) that should be used to compute
robustness for features in repeated measurements during the evaluation of
univariate importance. These types correspond to the types in Shrout and
Fleiss (1979). The default value is \code{1}.}

\item{stratification_method}{(\emph{optional}) Method for determining the
stratification threshold for creating survival groups. The actual,
model-dependent, threshold value is obtained from the development data, and
can afterwards be used to perform stratification on validation data.

The following stratification methods are available:
\itemize{
\item \code{median} (default): The median predicted value in the development cohort
is used to stratify the samples into two risk groups.
\item \code{fixed}: Samples are stratified based on the sample quantiles of the
predicted values. These quantiles are defined using the
\code{stratification_threshold} parameter.
\item \code{optimised}: Use maximally selected rank statistics to determine the
optimal threshold (Lausen and Schumacher, 1992; Hothorn et al., 2003) to
stratify samples into two optimally separated risk groups.
}

One or more stratification methods can be selected simultaneously.

This parameter is only relevant for \code{survival} outcomes.}

\item{stratification_threshold}{(\emph{optional}) Numeric value(s) signifying the
sample quantiles for stratification using the \code{fixed} method. The number of
risk groups will be the number of values +1.

The default value is \code{c(1/3, 2/3)}, which will yield two thresholds that
divide samples into three equally sized groups. If \code{fixed} is not among the
selected stratification methods, this parameter is ignored.

This parameter is only relevant for \code{survival} outcomes.}

\item{stratification_ensemble_method}{(\emph{optional}) Method for ensembling the
risk group assignments from different models for the same sample.

The following methods are available:
\itemize{
\item \code{ensemble_mean}: Risk groups are determined for each sample using the
threshold values of the different models in the ensemble. The risk groups
are treated as ordinal and encoded using integer values. The mean of the
encoded values is computed. The risk group bin containing the mean value is
then used as the ensemble-based risk group.
\item \code{ensemble_mode} (default): Risk groups are determined for each sample
using the threshold values of the different models in the ensemble. The most
commonly assigned risk group is then used as the ensemble-based risk group.
\item \code{median_threshold}: The median threshold value for each risk group is
determined from the threshold values of the different models in the
ensemble. The resulting threshold(s) are then applied to the ensemble
prediction of a sample to identify the ensemble-based risk group.
\item \code{mean_threshold}: Similar to \code{median_threshold}, but uses the mean
threshold value for each risk group.
}

This parameter is only relevant for \code{survival} outcomes.}

\item{time_max}{(\emph{optional}) Time point which is used as the benchmark for
e.g. cumulative risks generated by random forest, or the cutoff for Uno's
concordance index.

If \code{time_max} is not provided, but \code{evaluation_times} is, the largest value
of \code{evaluation_times} is used. If both are not provided, \code{time_max} is set
to the 98th percentile of the distribution of survival times for samples
with an event in the development data set.

This parameter is only relevant for \code{survival} outcomes.}

\item{evaluation_times}{(\emph{optional}) One or more time points that are used for
assessing calibration in survival problems. This is done as expected and
observed survival probabilities depend on time.

If unset, \code{evaluation_times} will be equal to \code{time_max}.

This parameter is only relevant for \code{survival} outcomes.}

\item{parallel_evaluation}{(\emph{optional}) Enable parallel processing for
hyperparameter optimisation. Defaults to \code{TRUE}. When set to \code{FALSE}, this
will disable the use of parallel processing while performing optimisation,
regardless of the settings of the \code{parallel} parameter.
\code{parallel_evaluation} is ignored if \code{parallel=FALSE}.}

\item{...}{Unused arguments.}
}
\value{
List of parameters related to model evaluation.
}
\description{
Internal function for parsing settings related to model evaluation
}
\references{
\enumerate{
\item Davison, A. C. & Hinkley, D. V. Bootstrap methods and their
application. (Cambridge University Press, 1997).
\item Efron, B. & Hastie, T. Computer Age Statistical Inference. (Cambridge
University Press, 2016).
\item Lausen, B. & Schumacher, M. Maximally Selected Rank Statistics.
Biometrics 48, 73 (1992).
\item Hothorn, T. & Lausen, B. On the exact distribution of maximally selected
rank statistics. Comput. Stat. Data Anal. 43, 121â€“137 (2003).
}
}
\keyword{internal}
